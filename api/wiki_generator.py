"""
Wiki ç”Ÿæˆé€»è¾‘æ¨¡å—ï¼ˆåŸæ±åŸå‘³å¤åˆ»æ—§ç³»ç»Ÿï¼‰

å°† wiki ç”Ÿæˆçš„æ ¸å¿ƒé€»è¾‘ç‹¬ç«‹å‡ºæ¥ï¼Œé¿å…åœ¨ task_queue.py ä¸­å¤„ç†å¤æ‚çš„å­—ç¬¦ä¸²æ“ä½œ
å®Œå…¨å¤åˆ»æ—§å‰ç«¯ç³»ç»Ÿçš„ç”Ÿæˆç®—æ³•å’Œ Prompt
"""

import json
import re
import logging
import time
import xml.etree.ElementTree as ET
from typing import Dict, List, Any, Optional
from api.config import get_model_config
from api.prompts import (
    WIKI_STRUCTURE_COMPREHENSIVE_PROMPT,
    WIKI_STRUCTURE_CONCISE_PROMPT,
    WIKI_CONTENT_PROMPT
)
# from api.mermaid_adapter import render_mermaid_in_markdown  # ä¸å†ä½¿ç”¨ï¼ŒMermaid æ¸²æŸ“ç”±å‰ç«¯å¤„ç†
import adalflow as adal

logger = logging.getLogger(__name__)


# è¯­è¨€æ˜ å°„è¡¨ï¼ˆå…¨å±€ï¼‰
LANGUAGE_MAP = {
    'en': 'English',
    'zh': 'Mandarin Chinese (ä¸­æ–‡)',
    'zh-tw': 'Traditional Chinese (ç¹é«”ä¸­æ–‡)',
    'ja': 'Japanese (æ—¥æœ¬èª)',
    'ko': 'Korean (í•œêµ­ì–´)',
    'kr': 'Korean (í•œêµ­ì–´)',  # å…¼å®¹æ—§ç³»ç»Ÿ
    'es': 'Spanish (EspaÃ±ol)',
    'fr': 'French (FranÃ§ais)',
    'pt-br': 'Brazilian Portuguese (PortuguÃªs Brasileiro)',
    'ru': 'Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)',
    'vi': 'Vietnamese (Tiáº¿ng Viá»‡t)',
    'ar': 'Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)'
}


def clean_html_from_markdown(content: str) -> str:
    """
    ä» markdown å†…å®¹ä¸­ç§»é™¤ HTML æ ‡ç­¾

    ç§»é™¤å¦‚ <details>, <summary> ç­‰ HTML æ ‡ç­¾ï¼Œåªä¿ç•™çº¯ markdown æ ¼å¼

    Args:
        content: åŸå§‹å†…å®¹ï¼ˆå¯èƒ½åŒ…å« HTMLï¼‰

    Returns:
        æ¸…ç†åçš„çº¯ markdown å†…å®¹
    """
    # ç§»é™¤ <details> å’Œ <summary> å—
    # æ¨¡å¼ï¼š<details>...<summary>...</summary>...\n</details>
    content = re.sub(
        r'<details>.*?<summary>.*?</summary>.*?\n</details>\n*',
        '',
        content,
        flags=re.DOTALL
    )

    # å¦‚æœè¿˜æœ‰å…¶ä»– HTML æ³¨é‡Šï¼Œä¹Ÿç§»é™¤
    content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)

    # ä¿®å¤å¯èƒ½çš„é‡å¤æ¢è¡Œ
    content = re.sub(r'\n{3,}', '\n\n', content)

    return content.strip()


def generate_wiki_structure(task: Dict[str, Any], documents_count: int, file_tree: str = '', readme: str = '') -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆ wiki ç»“æ„ï¼ˆé¡µé¢åˆ—è¡¨ï¼‰

    å®Œå…¨å¤åˆ»æ—§ç³»ç»Ÿçš„é€»è¾‘ï¼š
    - å…¨é¢å‹: 8-12 é¡µï¼Œå¸¦ sections
    - ç®€æ´å‹: 4-6 é¡µï¼Œæ‰å¹³ç»“æ„
    - ä½¿ç”¨ XML æ ¼å¼è¿”å›

    Args:
        task: ä»»åŠ¡ä¿¡æ¯
        documents_count: æ–‡æ¡£æ•°é‡
        file_tree: æ–‡ä»¶æ ‘ï¼ˆå¯é€‰ï¼‰
        readme: README å†…å®¹ï¼ˆå¯é€‰ï¼‰

    Returns:
        é¡µé¢ç»“æ„åˆ—è¡¨
    """
    task_id = task['task_id']
    repo_name = task['repo_name']
    owner = task.get('owner', '')
    language = task.get('language', 'zh')
    is_comprehensive = task.get('is_comprehensive', True)

    target_language = LANGUAGE_MAP.get(language, 'Mandarin Chinese (ä¸­æ–‡)')

    logger.info(f"[Task {task_id}] Generating structure - is_comprehensive: {is_comprehensive} (type: {type(is_comprehensive)})")

    # æ„å»ºå®Œå…¨å¤åˆ»æ—§ç³»ç»Ÿçš„ Prompt
    if is_comprehensive:
        logger.info(f"[Task {task_id}] Using COMPREHENSIVE structure prompt")
        # ========== å…¨é¢å‹ Prompt ==========
        structure_prompt = WIKI_STRUCTURE_COMPREHENSIVE_PROMPT.format(
            owner=owner,
            repo_name=repo_name,
            file_tree=file_tree if file_tree else '(File tree not available)',
            readme=readme if readme else '(README not available)',
            target_language=target_language
        )

    else:
        logger.info(f"[Task {task_id}] Using CONCISE structure prompt")
        # ========== ç®€æ´å‹ Prompt ==========
        structure_prompt = WIKI_STRUCTURE_CONCISE_PROMPT.format(
            owner=owner,
            repo_name=repo_name,
            file_tree=file_tree if file_tree else '(File tree not available)',
            readme=readme if readme else '(README not available)',
            target_language=target_language
        )

    # ========== æ–°å¢ï¼šToken æ£€æµ‹ + æ™ºèƒ½æ¨¡å‹é™çº§ ==========
    prompt_token_count = estimate_tokens(structure_prompt)
    provider = task['provider']
    model = task['model']

    logger.info(f"[Task {task_id}] ğŸ“Š Estimated prompt tokens: {prompt_token_count}")

    # å®šä¹‰å„æ¨¡å‹çš„ token é™åˆ¶ï¼ˆä¿ç•™å®‰å…¨ä½™é‡ï¼‰
    model_token_limits = {
        'anthropic/claude-opus-4': 150000,      # 200K - 50K ä½™é‡
        'anthropic/claude-3.5-sonnet': 150000,  # 200K - 50K ä½™é‡
        'anthropic/claude-haiku-4.5': 150000,   # 200K - 50K ä½™é‡ï¼ˆå®¹æ˜“è¶…é™çš„æ¨¡å‹ï¼‰
        'openai/gpt-4-turbo': 100000,           # 128K - 28K ä½™é‡
        'openai/gpt-4o': 100000,                # 128K - 28K ä½™é‡
        'default': 150000
    }

    current_limit = model_token_limits.get(model, model_token_limits['default'])

    # å¦‚æœè¶…è¿‡å½“å‰æ¨¡å‹çš„é™åˆ¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° Gemini 2.5 Flashï¼ˆæ”¯æŒ 1M tokenï¼‰
    if prompt_token_count > current_limit:
        logger.warning(
            f"[Task {task_id}] âš ï¸ Prompt too large for {model}: "
            f"{prompt_token_count} tokens > {current_limit} limit. "
            f"Auto-switching to google/gemini-2.5-flash"
        )
        provider = 'openrouter'  # Google Gemini é€šè¿‡ OpenRouter ä½¿ç”¨
        model = 'google/gemini-2.5-flash'  # 1M token ä¸Šé™

    # è°ƒç”¨ AI ç”Ÿæˆ
    generator_config = get_model_config(provider, model)
    generator = adal.Generator(
        model_client=generator_config["model_client"](),
        model_kwargs=generator_config["model_kwargs"]
    )

    # å¦‚æœå¼ºåˆ¶åˆ·æ–°ï¼Œæ·»åŠ éšæœºå™ªå£°ä»¥ç»•è¿‡ç¼“å­˜
    if task.get('force_refresh'):
        structure_prompt += f"\n\n<!-- Cache Buster: {time.time()} -->"
        logger.info(f"[Task {task_id}] ğŸ”„ Force refresh enabled: Added cache buster to structure prompt")

    logger.info(f"[Task {task_id}] Calling AI to generate structure (comprehensive={is_comprehensive}, model={model})...")
    structure_response = generator(prompt_kwargs={"input_str": structure_prompt})
    response_text = str(structure_response.data if hasattr(structure_response, 'data') else structure_response)

    # Log usage information if available and track cost
    try:
        if hasattr(structure_response, 'raw_response') and structure_response.raw_response:
            raw_resp = structure_response.raw_response
            if hasattr(raw_resp, 'usage') and raw_resp.usage:
                usage = raw_resp.usage
                prompt_tokens = getattr(usage, 'prompt_tokens', None)
                completion_tokens = getattr(usage, 'completion_tokens', None)
                total_tokens = getattr(usage, 'total_tokens', None)
                cost = getattr(usage, 'cost', None)

                log_parts = []
                if prompt_tokens is not None:
                    log_parts.append(f"prompt_tokens: {prompt_tokens}")
                if completion_tokens is not None:
                    log_parts.append(f"completion_tokens: {completion_tokens}")
                if total_tokens is not None:
                    log_parts.append(f"total_tokens: {total_tokens}")
                if cost is not None:
                    log_parts.append(f"cost: ${cost:.5f}")

                if log_parts:
                    logger.info(f"[Task {task_id}] Wiki structure generation - {', '.join(log_parts)}")

                # Track cost
                try:
                    from api.cost_tracker import get_cost_tracker
                    cost_tracker = get_cost_tracker(task_id)
                    # Even if cost is None (0), we should track tokens
                    cost_tracker.add_llm_cost(
                        prompt_tokens=prompt_tokens or 0,
                        completion_tokens=completion_tokens or 0,
                        total_tokens=total_tokens or 0,
                        cost=cost or 0.0
                    )
                except Exception as e:
                    logger.debug(f"[Task {task_id}] Could not track LLM cost: {e}")
    except Exception as e:
        logger.debug(f"[Task {task_id}] Could not extract usage info: {e}")

    logger.debug(f"[Task {task_id}] Raw structure response: {response_text[:500]}...")

    # è§£æ XML
    try:
        pages_structure = parse_wiki_structure_xml(response_text, task_id, is_comprehensive)
        logger.info(f"[Task {task_id}] âœ“ Parsed {len(pages_structure)} pages from structure")
        return pages_structure
    except Exception as e:
        logger.warning(f"[Task {task_id}] âœ— Failed to parse structure XML: {e}")
        logger.debug(f"[Task {task_id}] Failed XML text: {response_text[:1000]}")

        # å›é€€åˆ°é»˜è®¤ç»“æ„
        return get_default_structure(is_comprehensive, target_language)


def parse_wiki_structure_xml(xml_text: str, task_id: str, is_comprehensive: bool) -> List[Dict[str, Any]]:
    """
    è§£æ XML æ ¼å¼çš„ wiki ç»“æ„

    å®Œå…¨å¤åˆ»æ—§ç³»ç»Ÿçš„ XML è§£æé€»è¾‘
    """
    # æ¸…ç† XML æ–‡æœ¬
    xml_text = xml_text.strip()

    # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—
    if '```' in xml_text:
        xml_match = re.search(r'```(?:xml)?\s*(<wiki_structure>.*?</wiki_structure>)\s*```', xml_text, re.DOTALL)
        if xml_match:
            xml_text = xml_match.group(1)

    # æå– <wiki_structure> å†…å®¹
    if '<wiki_structure>' in xml_text and '</wiki_structure>' in xml_text:
        start = xml_text.find('<wiki_structure>')
        end = xml_text.find('</wiki_structure>') + len('</wiki_structure>')
        xml_text = xml_text[start:end]

    # è§£æ XML
    root = ET.fromstring(xml_text)

    # æå–åŸºæœ¬ä¿¡æ¯
    title = root.find('title')
    description = root.find('description')

    wiki_title = title.text if title is not None else 'Wiki'
    wiki_description = description.text if description is not None else 'Project documentation'

    # è§£æ pages
    pages = []
    pages_element = root.find('pages')

    if pages_element is not None:
        for page_elem in pages_element.findall('page'):
            page_id = page_elem.get('id', f'page-{len(pages) + 1}')

            page_data = {
                'id': page_id,
                'title': page_elem.find('title').text if page_elem.find('title') is not None else 'Untitled',
                'description': page_elem.find('description').text if page_elem.find('description') is not None else '',
                'importance': page_elem.find('importance').text if page_elem.find('importance') is not None else 'medium',
            }

            # æå– relevant_files
            relevant_files_elem = page_elem.find('relevant_files')
            if relevant_files_elem is not None:
                file_paths = [fp.text for fp in relevant_files_elem.findall('file_path') if fp.text]
                page_data['filePaths'] = file_paths
            else:
                page_data['filePaths'] = []

            # æå– related_pages
            related_pages_elem = page_elem.find('related_pages')
            if related_pages_elem is not None:
                related = [rp.text for rp in related_pages_elem.findall('related') if rp.text]
                page_data['relatedPages'] = related
            else:
                page_data['relatedPages'] = []

            # æå– parent_section
            parent_section_elem = page_elem.find('parent_section')
            if parent_section_elem is not None:
                page_data['parentSection'] = parent_section_elem.text

            pages.append(page_data)

    # è§£æ sectionsï¼ˆä»…å…¨é¢å‹ï¼‰
    sections = []
    if is_comprehensive:
        sections_element = root.find('sections')
        if sections_element is not None:
            for section_elem in sections_element.findall('section'):
                section_data = {
                    'id': section_elem.get('id', f'section-{len(sections) + 1}'),
                    'title': section_elem.find('title').text if section_elem.find('title') is not None else 'Untitled Section',
                }

                # æå–è¯¥åˆ†åŒºçš„é¡µé¢
                pages_elem = section_elem.find('pages')
                if pages_elem is not None:
                    section_data['pages'] = [pr.text for pr in pages_elem.findall('page_ref') if pr.text]
                else:
                    section_data['pages'] = []

                sections.append(section_data)

    logger.info(f"[Task {task_id}] Parsed {len(pages)} pages, {len(sections)} sections")

    # éªŒè¯é¡µé¢æ•°é‡å¹¶å¼ºåˆ¶æ‰§è¡Œçº¦æŸ
    if is_comprehensive:
        max_pages = 12
        min_pages = 8
        if len(pages) > max_pages:
            logger.warning(f"[Task {task_id}] âš ï¸ Model generated {len(pages)} pages, exceeding limit of {max_pages}. Truncating to {max_pages} pages.")
            pages = pages[:max_pages]
        elif len(pages) < min_pages:
            logger.warning(f"[Task {task_id}] âš ï¸ Model generated only {len(pages)} pages, below minimum of {min_pages}.")
    else:
        max_pages = 6
        min_pages = 4
        if len(pages) > max_pages:
            logger.warning(f"[Task {task_id}] âš ï¸ Model generated {len(pages)} pages, exceeding limit of {max_pages}. Truncating to {max_pages} pages.")
            pages = pages[:max_pages]
        elif len(pages) < min_pages:
            logger.warning(f"[Task {task_id}] âš ï¸ Model generated only {len(pages)} pages, below minimum of {min_pages}.")

    return pages


def get_default_structure(is_comprehensive: bool, language: str) -> List[Dict[str, Any]]:
    """
    è·å–é»˜è®¤çš„ wiki ç»“æ„ï¼ˆå½“è§£æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰
    """
    if is_comprehensive:
        return [
            {"id": "overview", "title": "æ¦‚è¿°" if 'Chinese' in language else "Overview", "importance": "high", "description": "é¡¹ç›®æ¦‚è¿°", "filePaths": []},
            {"id": "architecture", "title": "æ¶æ„" if 'Chinese' in language else "Architecture", "importance": "high", "description": "ç³»ç»Ÿæ¶æ„", "filePaths": []},
            {"id": "features", "title": "æ ¸å¿ƒåŠŸèƒ½" if 'Chinese' in language else "Core Features", "importance": "high", "description": "ä¸»è¦åŠŸèƒ½", "filePaths": []},
            {"id": "setup", "title": "å®‰è£…æŒ‡å—" if 'Chinese' in language else "Setup Guide", "importance": "medium", "description": "å¦‚ä½•å®‰è£…", "filePaths": []},
            {"id": "api", "title": "API å‚è€ƒ" if 'Chinese' in language else "API Reference", "importance": "medium", "description": "API æ–‡æ¡£", "filePaths": []},
        ]
    else:
        return [
            {"id": "overview", "title": "æ¦‚è¿°" if 'Chinese' in language else "Overview", "importance": "high", "description": "é¡¹ç›®æ¦‚è¿°", "filePaths": []},
            {"id": "getting-started", "title": "å¿«é€Ÿå¼€å§‹" if 'Chinese' in language else "Getting Started", "importance": "high", "description": "å¿«é€Ÿä¸Šæ‰‹", "filePaths": []},
            {"id": "usage", "title": "ä½¿ç”¨ç¤ºä¾‹" if 'Chinese' in language else "Usage Examples", "importance": "medium", "description": "ä½¿ç”¨æ–¹æ³•", "filePaths": []},
        ]


def estimate_tokens(text: str) -> int:
    """
    ç²—ç•¥ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæ£€æµ‹ï¼‰
    åŸºäºç»éªŒæ³•åˆ™ï¼šå¹³å‡ 1 ä¸ª token â‰ˆ 4 ä¸ªå­—ç¬¦

    Args:
        text: è¾“å…¥æ–‡æœ¬

    Returns:
        ä¼°ç®—çš„ token æ•°
    """
    return len(text) // 4


def generate_page_content(task: Dict[str, Any], page_info: Dict[str, Any], relevant_code: str = "", file_paths: List[str] = None) -> str:
    """
    ç”Ÿæˆå•ä¸ªé¡µé¢çš„å†…å®¹

    å®Œå…¨å¤åˆ»æ—§ç³»ç»Ÿçš„é¡µé¢ç”Ÿæˆ Prompt

    Args:
        task: ä»»åŠ¡ä¿¡æ¯
        page_info: é¡µé¢ä¿¡æ¯ï¼ˆid, title, description, filePathsï¼‰
        relevant_code: RAGæ£€ç´¢åˆ°çš„ç›¸å…³ä»£ç 
        file_paths: ä» RAG æ£€ç´¢å¾—åˆ°çš„å®é™…æ–‡ä»¶è·¯å¾„åˆ—è¡¨

    Returns:
        Markdown å†…å®¹ï¼ˆä¸åŒ…å« HTML æ ‡ç­¾ï¼‰
    """
    task_id = task['task_id']
    page_title = page_info.get('title', 'Untitled')
    description = page_info.get('description', 'Technical documentation')

    # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ file_pathsï¼Œå¦åˆ™ä½¿ç”¨ page_info ä¸­çš„
    if file_paths is None:
        file_paths = page_info.get('filePaths', [])

    language = task.get('language', 'zh')

    target_language = LANGUAGE_MAP.get(language, 'Mandarin Chinese (ä¸­æ–‡)')

    # ç”Ÿæˆæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆmarkdown æ ¼å¼ï¼‰
    file_paths_md = '\n'.join([f"- {fp}" for fp in file_paths]) if file_paths else '(No specific files provided)'

    # æ·»åŠ å®é™…ä»£ç å†…å®¹
    code_context = ""
    if relevant_code:
        code_context = f"""

RELEVANT SOURCE CODE FROM THE PROJECT:
======================================
{relevant_code[:15000]}
======================================

Use the above source code as the basis for generating the wiki content.
"""
    else:
        code_context = "\n\nWARNING: No source code was provided. This should not happen. The wiki page must be based on actual source code."

    # ========== å®Œå…¨å¤åˆ»æ—§ç³»ç»Ÿçš„é¡µé¢å†…å®¹ Prompt ==========
    content_prompt = WIKI_CONTENT_PROMPT.replace(
        "__PAGE_TITLE__", page_title
    ).replace(
        "__CODE_CONTEXT__", code_context
    ).replace(
        "__FILE_PATHS_MD__", file_paths_md
    ).replace(
        "__TARGET_LANGUAGE__", target_language
    )

    # ========== æ–°å¢ï¼šToken æ£€æµ‹ + æ™ºèƒ½æ¨¡å‹é™çº§ ==========
    prompt_token_count = estimate_tokens(content_prompt)
    provider = task['provider']
    model = task['model']

    logger.info(f"[Task {task_id}] ğŸ“Š Estimated prompt tokens: {prompt_token_count}")

    # å®šä¹‰å„æ¨¡å‹çš„ token é™åˆ¶ï¼ˆä¿ç•™å®‰å…¨ä½™é‡ï¼‰
    model_token_limits = {
        'anthropic/claude-opus-4': 150000,      # 200K - 50K ä½™é‡
        'anthropic/claude-3.5-sonnet': 150000,  # 200K - 50K ä½™é‡
        'anthropic/claude-haiku-4.5': 150000,   # 200K - 50K ä½™é‡ï¼ˆå®¹æ˜“è¶…é™çš„æ¨¡å‹ï¼‰
        'openai/gpt-4-turbo': 100000,           # 128K - 28K ä½™é‡
        'openai/gpt-4o': 100000,                # 128K - 28K ä½™é‡
        'default': 150000
    }

    current_limit = model_token_limits.get(model, model_token_limits['default'])

    # å¦‚æœè¶…è¿‡å½“å‰æ¨¡å‹çš„é™åˆ¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° Gemini 2.5 Flashï¼ˆæ”¯æŒ 1M tokenï¼‰
    if prompt_token_count > current_limit:
        logger.warning(
            f"[Task {task_id}] âš ï¸ Prompt too large for {model}: "
            f"{prompt_token_count} tokens > {current_limit} limit. "
            f"Auto-switching to google/gemini-2.5-flash"
        )
        provider = 'openrouter'  # Google Gemini é€šè¿‡ OpenRouter ä½¿ç”¨
        model = 'google/gemini-2.5-flash'  # 1M token ä¸Šé™

    generator_config = get_model_config(provider, model)
    generator = adal.Generator(
        model_client=generator_config["model_client"](),
        model_kwargs=generator_config["model_kwargs"]
    )

    # å¦‚æœå¼ºåˆ¶åˆ·æ–°ï¼Œæ·»åŠ éšæœºå™ªå£°ä»¥ç»•è¿‡ç¼“å­˜
    if task.get('force_refresh'):
        content_prompt += f"\n\n<!-- Cache Buster: {time.time()} -->"
        logger.info(f"[Task {task_id}] ğŸ”„ Force refresh enabled: Added cache buster to content prompt for page '{page_title}'")

    logger.info(f"[Task {task_id}] Generating content for page: {page_title}")
    content_response = generator(prompt_kwargs={"input_str": content_prompt})
    content = str(content_response.data if hasattr(content_response, 'data') else content_response)

    # Log usage information if available and track cost
    try:
        # Check if usage is directly available on content_response (GeneratorOutput)
        usage = getattr(content_response, 'usage', None)

        # Fallback: check in raw_response if usage is not directly available
        if not usage and hasattr(content_response, 'raw_response'):
            raw_resp = content_response.raw_response
            if hasattr(raw_resp, 'usage'):
                usage = raw_resp.usage

        # Debug log for content_response structure
        if not usage:
            logger.debug(f"[Task {task_id}] No usage found. content_response type: {type(content_response)}")
            logger.debug(f"[Task {task_id}] content_response dir: {dir(content_response)}")
            if hasattr(content_response, 'raw_response'):
                logger.debug(f"[Task {task_id}] raw_response type: {type(content_response.raw_response)}")
                logger.debug(f"[Task {task_id}] raw_response dir: {dir(content_response.raw_response)}")

        if usage:
            prompt_tokens = getattr(usage, 'prompt_tokens', None)
            completion_tokens = getattr(usage, 'completion_tokens', None)
            total_tokens = getattr(usage, 'total_tokens', None)
            cost = getattr(usage, 'cost', None)

            log_parts = []
            if prompt_tokens is not None:
                log_parts.append(f"prompt_tokens: {prompt_tokens}")
            if completion_tokens is not None:
                log_parts.append(f"completion_tokens: {completion_tokens}")
            if total_tokens is not None:
                log_parts.append(f"total_tokens: {total_tokens}")
            if cost is not None:
                log_parts.append(f"cost: ${cost:.5f}")

            if log_parts:
                logger.info(f"[Task {task_id}] Page '{page_title}' content generation - {', '.join(log_parts)}")

            # Track cost
            try:
                from api.cost_tracker import get_cost_tracker
                cost_tracker = get_cost_tracker(task_id)
                # Even if cost is None (0), we should track tokens
                cost_tracker.add_llm_cost(
                    prompt_tokens=prompt_tokens or 0,
                    completion_tokens=completion_tokens or 0,
                    total_tokens=total_tokens or 0,
                    cost=cost or 0.0
                )
            except Exception as e:
                logger.debug(f"[Task {task_id}] Could not track LLM cost for page '{page_title}': {e}")
    except Exception as e:
        logger.debug(f"[Task {task_id}] Could not extract usage info for page '{page_title}': {e}")

    logger.info(f"[Task {task_id}] âœ“ Generated {len(content)} chars for page: {page_title}")

    # æ¸…ç† HTML æ ‡ç­¾ï¼ˆé˜²æ­¢ <details> ç­‰ HTML æ··å…¥ markdownï¼‰
    content = clean_html_from_markdown(content)

    return content


def generate_wiki(task: Dict[str, Any], rag: Any, progress_callback=None) -> tuple[Dict[str, Any], int]:
    """
    ç”Ÿæˆå®Œæ•´çš„ Wikiï¼ˆç»“æ„ + æ‰€æœ‰é¡µé¢å†…å®¹ï¼‰

    å®Œå…¨å¤åˆ»æ—§ç³»ç»Ÿçš„ç”Ÿæˆæµç¨‹

    Args:
        task: ä»»åŠ¡ä¿¡æ¯
        rag: RAG å®ä¾‹ï¼ˆåŒ…å«æ–‡æ¡£æ•°æ®ï¼‰
        progress_callback: å¯é€‰çš„è¿›åº¦å›è°ƒå‡½æ•° (progress_pct, current_stage, detail_message)

    Returns:
        tuple: (Wiki ç»“æ„, æ–‡æ¡£æ•°é‡)
    """
    task_id = task['task_id']
    repo_name = task['repo_name']

    logger.info(f"[Task {task_id}] Starting wiki generation for: {repo_name}")

    # è·å–æ–‡æ¡£æ•°é‡
    try:
        documents_count = len(rag.documents) if hasattr(rag, 'documents') and rag.documents else 0
    except:
        documents_count = 0

    logger.info(f"[Task {task_id}] Repository has {documents_count} documents")

    # æ„å»ºæ–‡ä»¶æ ‘å’Œæå– README
    file_tree = ""
    readme = ""

    # logger.info(f"[Task {task_id}] ğŸ” Checking RAG documents...")
    # logger.info(f"[Task {task_id}] - hasattr(rag, 'documents'): {hasattr(rag, 'documents')}")

    if hasattr(rag, 'documents'):
        # logger.info(f"[Task {task_id}] - rag.documents is None: {rag.documents is None}")
        if rag.documents:
            # logger.info(f"[Task {task_id}] - len(rag.documents): {len(rag.documents)}")
            # logger.info(f"[Task {task_id}] - type(rag.documents[0]): {type(rag.documents[0]) if rag.documents else 'N/A'}")

            # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ç»“æ„
            if rag.documents:
                first_doc = rag.documents[0]
                # logger.info(f"[Task {task_id}] - First doc attributes: {dir(first_doc)}")
                # logger.info(f"[Task {task_id}] - hasattr(first_doc, 'meta_data'): {hasattr(first_doc, 'meta_data')}")
                # logger.info(f"[Task {task_id}] - hasattr(first_doc, 'text'): {hasattr(first_doc, 'text')}")

                if hasattr(first_doc, 'meta_data'):
                    pass
                    # logger.info(f"[Task {task_id}] - first_doc.meta_data: {first_doc.meta_data}")

    if hasattr(rag, 'documents') and rag.documents:
        # ç”Ÿæˆæ–‡ä»¶æ ‘
        file_paths = []
        for i, doc in enumerate(rag.documents):
            if hasattr(doc, 'meta_data') and doc.meta_data:
                file_path = doc.meta_data.get('file_path', '')
                if file_path:
                    file_paths.append(file_path)
                    if i < 3:  # åªæ‰“å°å‰3ä¸ª
                        pass
                        # logger.info(f"[Task {task_id}] - Doc {i} file_path: {file_path}")

        file_tree = "\n".join(sorted(file_paths))
        logger.info(f"[Task {task_id}] âœ… Generated file tree with {len(file_paths)} files")

        if len(file_paths) == 0:
            logger.error(f"[Task {task_id}] âŒ WARNING: No file paths extracted from {len(rag.documents)} documents!")

        # æå– README å†…å®¹
        for doc in rag.documents:
            if hasattr(doc, 'meta_data') and doc.meta_data:
                file_path = doc.meta_data.get('file_path', '').lower()
                if 'readme' in file_path:
                    readme = str(doc.text) if hasattr(doc, 'text') else str(doc)
                    logger.info(f"[Task {task_id}] âœ… Found README file: {file_path}")
                    logger.info(f"[Task {task_id}] - README length: {len(readme)} chars")
                    break
    else:
        logger.error(f"[Task {task_id}] âŒ CRITICAL: RAG has no documents!")

    # Step 1: ç”Ÿæˆ Wiki ç»“æ„
    logger.info(f"[Task {task_id}] Step 1: Generating wiki structure...")
    pages_structure = generate_wiki_structure(task, documents_count, file_tree, readme)

    # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆç»“æ„ç”Ÿæˆå®Œæˆï¼‰
    if progress_callback:
        progress_callback(70, 'structure', f'Wiki structure generated with {len(pages_structure)} pages')

    # Step 2: ä¸ºæ¯ä¸ªé¡µé¢ç”Ÿæˆå†…å®¹
    logger.info(f"[Task {task_id}] Step 2: Generating content for {len(pages_structure)} pages...")

    for i, page in enumerate(pages_structure):
        page_title = page['title']
        logger.info(f"[Task {task_id}] Generating page {i+1}/{len(pages_structure)}: {page_title}")

        # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
        if progress_callback:
            progress_pct = 70 + int((25 * i) / len(pages_structure))
            detail_msg = f'Generating page {i+1}/{len(pages_structure)}: {page_title}'
            progress_callback(progress_pct, 'pages', detail_msg)

        try:
            # ä½¿ç”¨ RAG retriever æ£€ç´¢ç›¸å…³ä»£ç å’Œæ–‡ä»¶è·¯å¾„
            relevant_code = ""
            file_paths = []

            # logger.info(f"[Task {task_id}] ğŸ” Starting code retrieval for page: {page['title']}")
            # logger.info(f"[Task {task_id}] - hasattr(rag, 'retriever'): {hasattr(rag, 'retriever')}")

            if hasattr(rag, 'retriever'):
                # logger.info(f"[Task {task_id}] - rag.retriever is None: {rag.retriever is None}")
                # logger.info(f"[Task {task_id}] - type(rag.retriever): {type(rag.retriever)}")
                pass

            if hasattr(rag, 'retriever') and rag.retriever:
                try:
                    # æ ¹æ®é¡µé¢æ ‡é¢˜å’Œæè¿°æ£€ç´¢ç›¸å…³ä»£ç 
                    query = f"{page.get('title', '')} {page.get('description', '')}"
                    # logger.info(f"[Task {task_id}] ğŸ“‹ Retrieval query: {query}")

                    # ä½¿ç”¨ retriever çš„ call æ–¹æ³•
                    # logger.info(f"[Task {task_id}] ğŸš€ Calling rag.retriever.call()")
                    retrieved_docs = rag.retriever.call(input=query, top_k=10)

                    # logger.info(f"[Task {task_id}] - Retrieved docs type: {type(retrieved_docs)}")
                    # logger.info(f"[Task {task_id}] - Retrieved docs count: {len(retrieved_docs) if retrieved_docs else 0}")

                    if retrieved_docs and len(retrieved_docs) > 0:
                        # æ£€æŸ¥ç¬¬ä¸€ä¸ªè¿”å›æ–‡æ¡£çš„ç»“æ„ï¼ˆRetrieverOutputï¼‰
                        first_output = retrieved_docs[0]
                        # logger.info(f"[Task {task_id}] - First output type: {type(first_output)}")
                        # logger.info(f"[Task {task_id}] - First output full object: {first_output}")
                        # logger.info(f"[Task {task_id}] - hasattr 'documents': {hasattr(first_output, 'documents')}")
                        # logger.info(f"[Task {task_id}] - hasattr 'doc_indices': {hasattr(first_output, 'doc_indices')}")
                        # logger.info(f"[Task {task_id}] - hasattr 'doc_scores': {hasattr(first_output, 'doc_scores')}")

                        code_snippets = []

                        # RetrieverOutput ä½¿ç”¨ doc_indices æŒ‡å‘ rag.documents
                        for idx_out, retriever_output in enumerate(retrieved_docs):
                            # logger.info(f"[Task {task_id}] - Processing RetrieverOutput #{idx_out}")

                            # ä½¿ç”¨ doc_indices ä» rag.documents æå–å®é™…æ–‡æ¡£
                            if hasattr(retriever_output, 'doc_indices') and retriever_output.doc_indices:
                                doc_indices = retriever_output.doc_indices
                                # logger.info(f"[Task {task_id}] âœ… Found {len(doc_indices)} doc_indices")

                                # ä» rag.documents ä¸­æå–
                                if hasattr(rag, 'documents') and rag.documents:
                                    for doc_idx in doc_indices:
                                        if 0 <= doc_idx < len(rag.documents):
                                            doc = rag.documents[doc_idx]

                                            # æå–æ–‡æœ¬å†…å®¹
                                            doc_text = None
                                            if hasattr(doc, 'text'):
                                                doc_text = str(doc.text)
                                            elif isinstance(doc, dict) and 'text' in doc:
                                                doc_text = str(doc['text'])
                                            elif isinstance(doc, str):
                                                doc_text = doc

                                            if doc_text:
                                                code_snippets.append(doc_text)
                                                if len(code_snippets) == 1:
                                                    pass
                                                    # logger.info(f"[Task {task_id}] - First doc text length: {len(doc_text)} chars")

                                            # æå–æ–‡ä»¶è·¯å¾„
                                            file_path = None
                                            if hasattr(doc, 'meta_data') and doc.meta_data:
                                                file_path = doc.meta_data.get('file_path', '')
                                                if len(file_paths) == 0:
                                                    pass
                                                    # logger.info(f"[Task {task_id}] - First doc meta_data: {doc.meta_data}")
                                            elif isinstance(doc, dict) and 'meta_data' in doc:
                                                file_path = doc['meta_data'].get('file_path', '')

                                            if file_path and file_path not in file_paths:
                                                file_paths.append(file_path)
                                                # logger.info(f"[Task {task_id}] âœ… File path #{len(file_paths)}: {file_path}")

                                    # logger.info(f"[Task {task_id}] âœ… Extracted {len(code_snippets)} code snippets from {len(file_paths)} files")
                                else:
                                    logger.error(f"[Task {task_id}] âŒ rag.documents not available!")
                            else:
                                logger.warning(f"[Task {task_id}] âš ï¸ No doc_indices in RetrieverOutput")

                        if code_snippets:
                            relevant_code = "\n\n---\n\n".join(code_snippets[:5])  # æœ€å¤š5ä¸ªä»£ç ç‰‡æ®µ
                            logger.info(f"[Task {task_id}] âœ… Retrieved {len(code_snippets)} code snippets from {len(file_paths)} files for page: {page['title']}")
                        else:
                            logger.warning(f"[Task {task_id}] âš ï¸ No valid code snippets extracted for page: {page['title']}")
                    else:
                        logger.warning(f"[Task {task_id}] âš ï¸ Retriever returned no documents for page: {page['title']}")
                except Exception as e:
                    logger.error(f"[Task {task_id}] âŒ Failed to retrieve code for page {page['title']}: {e}", exc_info=True)
            else:
                logger.warning(f"[Task {task_id}] âš ï¸ RAG retriever not available")

            # ä¼ é€’æ–‡ä»¶è·¯å¾„ç»™å†…å®¹ç”Ÿæˆå‡½æ•°
            content = generate_page_content(task, page, relevant_code, file_paths)

            # âš ï¸ ä¸å†åœ¨ç”Ÿæˆæ—¶æ¸²æŸ“ Mermaid å›¾è¡¨
            # Mermaid ä»£ç å—ä¿ç•™åœ¨ Markdown ä¸­ï¼Œç”±å‰ç«¯ä½¿ç”¨ react-markdown å¤„ç†
            # å‰ç«¯ä¼šè‡ªåŠ¨æ£€æµ‹ ```mermaid ... ``` ä»£ç å—å¹¶æ¸²æŸ“
            # content = render_mermaid_in_markdown(content, task_id=task_id)

            page['content'] = content
        except Exception as e:
            logger.error(f"[Task {task_id}] Failed to generate content for page {page['title']}: {e}")
            page['content'] = f"# {page['title']}\n\nç”Ÿæˆå¤±è´¥: {str(e)}"

    # æ„å»ºæœ€ç»ˆç»“æ„
    wiki_structure = {
        'title': f"{repo_name} Wiki",
        'description': f"Documentation for {repo_name}",
        'pages': pages_structure
    }

    logger.info(f"[Task {task_id}] âœ… Wiki generation complete!")

    return wiki_structure, documents_count
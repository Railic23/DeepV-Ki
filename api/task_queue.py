"""
Wiki ç”Ÿæˆä»»åŠ¡é˜Ÿåˆ—ç®¡ç†æ¨¡å—

è´Ÿè´£ï¼š
- ç®¡ç†å¼‚æ­¥wikiç”Ÿæˆä»»åŠ¡
- æŒä¹…åŒ–ä»»åŠ¡çŠ¶æ€åˆ°SQLite
- åå°å¤„ç†ä»»åŠ¡é˜Ÿåˆ—
- æä¾›ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢æ¥å£
- ä»»åŠ¡è¶…æ—¶æ§åˆ¶
"""

import logging
import uuid
import threading
import time
import os
import sqlite3
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime
from dotenv import load_dotenv
from api.gitlab_db import get_gitlab_db
from api.data_pipeline import download_repo, read_all_documents
from api.rag import RAG
from api.exceptions import TaskTimeoutError

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä¸ gitlab_client.py ä¿æŒä¸€è‡´ï¼‰
load_dotenv(Path(__file__).parent.parent / '.env')

logger = logging.getLogger(__name__)


class TaskQueueManager:
    """
    Wiki ç”Ÿæˆä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    - åˆ›å»ºå’Œè·Ÿè¸ªä»»åŠ¡
    - ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—
    - åå°å¤„ç†ä»»åŠ¡
    - æ›´æ–°ä»»åŠ¡è¿›åº¦
    - å…¨å±€å¹¶å‘æ§åˆ¶ï¼ˆåŒæ—¶åªèƒ½å¤„ç† 1 ä¸ªä»»åŠ¡ï¼‰
    - æ—¥æœŸé™åˆ¶ï¼ˆæ¯ä¸ªé¡¹ç›®æ¯å¤©åªèƒ½æˆåŠŸç”Ÿæˆ 1 æ¬¡ï¼‰
    """

    def __init__(self, max_concurrent_tasks: int = 1, poll_interval: int = 2):
        """
        åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨

        Args:
            max_concurrent_tasks: æœ€å¤šåŒæ—¶å¤„ç†çš„ä»»åŠ¡æ•°ï¼ˆé»˜è®¤1ä¸ªï¼Œå…¨å±€é™åˆ¶ï¼‰
            poll_interval: åå°çº¿ç¨‹è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
        """
        self.db = get_gitlab_db()
        self.max_concurrent_tasks = max_concurrent_tasks  # å…¨å±€é™åˆ¶ï¼šæ”¹ä¸º 1
        self.global_execution_lock = threading.Lock()  # å…¨å±€æ‰§è¡Œé”
        self.poll_interval = poll_interval
        self.active_tasks = {}  # è®°å½•å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡
        self.worker_thread = None
        self.running = False

    def create_task(
        self,
        repo_url: str,
        repo_type: str,
        owner: str,
        repo_name: str,
        provider: str,
        model: str,
        language: str,
        is_comprehensive: bool = True,
        excluded_dirs: Optional[str] = None,
        excluded_files: Optional[str] = None,
        included_dirs: Optional[str] = None,
        included_files: Optional[str] = None,
        access_token: Optional[str] = None,
        force_refresh: bool = False
    ) -> str:
        """
        åˆ›å»ºä¸€ä¸ªæ–°çš„wikiç”Ÿæˆä»»åŠ¡

        Args:
            repo_url: ä»“åº“URL
            repo_type: ä»“åº“ç±»å‹ (github, gitlab, bitbucket, gerrit)
            owner: ä»“åº“æ‰€æœ‰è€…
            repo_name: ä»“åº“åç§°
            provider: AIæä¾›å•† (google, openai, openrouter, etc.)
            model: æ¨¡å‹åç§°
            language: ç”Ÿæˆè¯­è¨€
            is_comprehensive: æ˜¯å¦ç”Ÿæˆå…¨é¢çš„wiki
            excluded_dirs: æ’é™¤çš„ç›®å½•
            excluded_files: æ’é™¤çš„æ–‡ä»¶
            included_dirs: åŒ…å«çš„ç›®å½•
            included_files: åŒ…å«çš„æ–‡ä»¶
            access_token: è®¿é—®ä»¤ç‰Œï¼ˆç”¨äºç§æœ‰ä»“åº“ï¼‰

        Returns:
            ä»»åŠ¡ID æˆ– å·²å­˜åœ¨çš„ä»»åŠ¡IDï¼ˆå¦‚æœé˜Ÿåˆ—ä¸­å­˜åœ¨ç›¸åŒé¡¹ç›®çš„ä»»åŠ¡ï¼‰
        """
        # æ­¥éª¤ 1: è·å–æˆ–åˆ›å»ºé¡¹ç›®è®°å½•ï¼ˆé¡¹ç›®ç»´åº¦ç®¡ç†ï¼‰
        project_key = f"{repo_type}:{owner}/{repo_name}"
        project = self.db.get_or_create_wiki_project(
            repo_url=repo_url,
            repo_type=repo_type,
            owner=owner,
            repo_name=repo_name
        )

        # æ­¥éª¤ 2: æ£€æŸ¥é¡¹ç›®å½“å‰çŠ¶æ€
        if project['status'] == 'generating':
            logger.info(f"âš ï¸ é¡¹ç›® {project_key} æ­£åœ¨ç”Ÿæˆä¸­ï¼Œä»»åŠ¡ID: {project['current_task_id']}")
            return project['current_task_id']

        # æ­¥éª¤ 3: åˆ›å»ºæ–°ä»»åŠ¡
        task_id = str(uuid.uuid4())
        logger.info(f"âœ… ä¸ºé¡¹ç›® {project_key} åˆ›å»ºæ–°ä»»åŠ¡: {task_id}")

        # æ­¥éª¤ 4: ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        success = self.db.create_wiki_generation_task(
            task_id=task_id,
            repo_url=repo_url,
            repo_type=repo_type,
            owner=owner,
            repo_name=repo_name,
            provider=provider,
            model=model,
            language=language,
            is_comprehensive=is_comprehensive,
            excluded_dirs=excluded_dirs,
            excluded_files=excluded_files,
            included_dirs=included_dirs,
            included_files=included_files,
            access_token=access_token,
            project_key=project_key,
            force_refresh=force_refresh
        )

        if success:
            # æ­¥éª¤ 5: æ›´æ–°é¡¹ç›®çŠ¶æ€ä¸º generating
            self.db.update_wiki_project_status(project_key, 'generating', task_id)

            logger.info(f"âœ… ä»»åŠ¡å·²åˆ›å»º: {task_id} (é¡¹ç›®: {project_key})")

            # æ­¥éª¤ 6: å¦‚æœworkerçº¿ç¨‹æ²¡æœ‰è¿è¡Œï¼Œå¯åŠ¨å®ƒ
            if not self.running:
                self.start()
            return task_id
        else:
            logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {project_key}")
            raise Exception("Failed to create task in database")

    def _check_duplicate_task(self, owner: str, repo_name: str) -> Optional[Dict[str, Any]]:
        """
        æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒé¡¹ç›®çš„ä»»åŠ¡ï¼ˆé¡¹ç›®çº§åˆ«å»é‡ï¼‰

        é¡¹ç›®çº§åˆ«å»é‡ï¼šåŒä¸€ä¸ªé¡¹ç›®ï¼ˆowner/repo_nameï¼‰ï¼Œä¸åŒç”¨æˆ·åªå…±äº«åŒä¸€ä¸ªä»»åŠ¡

        æ£€æŸ¥ä¼˜å…ˆçº§ï¼š
        1. queued/processing çŠ¶æ€çš„ä»»åŠ¡ â†’ ç›´æ¥å¤ç”¨ï¼ˆæ­£åœ¨å¤„ç†æˆ–ç­‰å¾…å¤„ç†ï¼‰
        2. 24å°æ—¶å†…completedçš„ä»»åŠ¡ â†’ å¤ç”¨ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        3. å…¶ä»–æƒ…å†µ â†’ åˆ›å»ºæ–°ä»»åŠ¡

        Args:
            owner: ä»“åº“æ‰€æœ‰è€…
            repo_name: ä»“åº“åç§°

        Returns:
            å¦‚æœå­˜åœ¨å¯å¤ç”¨çš„ä»»åŠ¡åˆ™è¿”å›ä»»åŠ¡ä¿¡æ¯ï¼Œå¦åˆ™è¿”å›None
        """
        try:
            import sqlite3
            from datetime import datetime, timedelta

            with sqlite3.connect(self.db.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                # é¦–å…ˆæ£€æŸ¥ queued å’Œ processing çŠ¶æ€ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                cursor.execute('''
                    SELECT * FROM wiki_generation_tasks
                    WHERE owner = ? AND repo_name = ?
                    AND status IN ('queued', 'processing')
                    ORDER BY created_at DESC
                    LIMIT 1
                ''', (owner, repo_name))
                row = cursor.fetchone()
                if row:
                    logger.info(f"âš ï¸ é¡¹ç›® {owner}/{repo_name} å·²åœ¨å¤„ç†ä¸­ï¼Œå¤ç”¨ä»»åŠ¡ (status: {row['status']}, task_id: {row['task_id']})")
                    return dict(row)

                # ä¸å¤ç”¨å·²å®Œæˆçš„ä»»åŠ¡ï¼Œå…è®¸ç”¨æˆ·é‡æ–°ç”Ÿæˆï¼ˆåº”å¯¹ä»£ç æ›´æ–°ï¼‰
                logger.info(f"âœ“ é¡¹ç›® {owner}/{repo_name} æ²¡æœ‰è¿›è¡Œä¸­çš„ä»»åŠ¡ï¼Œå…è®¸åˆ›å»ºæ–°ä»»åŠ¡")
                return None

        except Exception as e:
            logger.warning(f"æ£€æŸ¥é‡å¤ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            return None

    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡ä¿¡æ¯å­—å…¸æˆ– None
        """
        task = self.db.get_task(task_id)
        if task:
            # è½¬æ¢ä¸ºå‰ç«¯å‹å¥½çš„æ ¼å¼
            return {
                'task_id': task['task_id'],
                'status': task['status'],
                'progress': task['progress'],
                'message': task['message'],
                'repo_url': task['repo_url'],
                'repo_type': task['repo_type'],
                'owner': task['owner'],
                'repo_name': task['repo_name'],
                'provider': task['provider'],
                'model': task['model'],
                'language': task['language'],
                'result': task.get('result'),
                'error_message': task.get('error_message'),
                'created_at': task['created_at'],
                'completed_at': task.get('completed_at'),
            }
        return None

    def start(self):
        """å¯åŠ¨åå°workerçº¿ç¨‹å¤„ç†ä»»åŠ¡"""
        if self.running:
            logger.warning("Worker thread is already running")
            return

        self.running = True
        self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
        self.worker_thread.start()
        logger.info("âœ… ä»»åŠ¡é˜Ÿåˆ—workerçº¿ç¨‹å·²å¯åŠ¨")

    def stop(self):
        """åœæ­¢åå°workerçº¿ç¨‹"""
        self.running = False
        if self.worker_thread and self.worker_thread.is_alive():
            self.worker_thread.join(timeout=10)
        logger.info("ğŸ›‘ ä»»åŠ¡é˜Ÿåˆ—workerçº¿ç¨‹å·²åœæ­¢")

    def _worker_loop(self):
        """åå°workerçº¿ç¨‹ä¸»å¾ªç¯"""
        logger.info("ğŸ”„ ä»»åŠ¡é˜Ÿåˆ—workerçº¿ç¨‹å¯åŠ¨ï¼Œå¼€å§‹å¤„ç†ä»»åŠ¡...")

        while self.running:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤„ç†æ§½ä½
                active_count = len(self.active_tasks)
                if active_count < self.max_concurrent_tasks:
                    # è·å–ä¸‹ä¸€ä¸ªæ’é˜Ÿçš„ä»»åŠ¡
                    queued_tasks = self.db.get_queued_tasks(limit=1)
                    if queued_tasks:
                        task = queued_tasks[0]
                        task_id = task['task_id']
                        logger.info(f"ğŸ“‹ å¼€å§‹å¤„ç†ä»»åŠ¡: {task_id}")

                        # è®°å½•æ­£åœ¨å¤„ç†çš„ä»»åŠ¡
                        self.active_tasks[task_id] = True

                        # åœ¨çº¿ç¨‹ä¸­å¤„ç†ä»»åŠ¡
                        task_thread = threading.Thread(
                            target=self._process_task,
                            args=(task,),
                            daemon=False
                        )
                        task_thread.start()

                # ç­‰å¾…ä¸€æ®µæ—¶é—´åå†æ£€æŸ¥
                time.sleep(self.poll_interval)

            except Exception as e:
                logger.error(f"âŒ Workerçº¿ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
                time.sleep(self.poll_interval)

    def _process_task(self, task: Dict[str, Any]):
        """
        å¤„ç†å•ä¸ªä»»åŠ¡

        Args:
            task: ä»»åŠ¡ä¿¡æ¯å­—å…¸
        """
        task_id = task['task_id']

        try:
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼ˆåœ¨å‡½æ•°å¼€å¤´å¯¼å…¥ï¼Œé¿å… UnboundLocalErrorï¼‰
            import os
            from pathlib import Path

            logger.info(f"ğŸš€ [Task {task_id}] å¼€å§‹å¤„ç†...")

            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.db.update_task_status(
                task_id=task_id,
                status='processing',
                progress=0,
                message='Starting wiki generation...'
            )

            # ç¬¬1é˜¶æ®µï¼šä¸‹è½½ä»“åº“ï¼ˆè¿›åº¦0-20%ï¼‰
            logger.info(f"[Task {task_id}] Stage 1: Downloading repository...")
            self.db.update_task_status(
                task_id=task_id,
                status='processing',
                progress=5,
                message='Downloading repository...'
            )

            # ğŸ” ä»ç¯å¢ƒå˜é‡è·å– access_tokenï¼ˆå¦‚æœä»»åŠ¡ä¸­æ²¡æœ‰æä¾›ï¼‰
            # å‚è€ƒ gitlab_client.py çš„åšæ³•ï¼Œç›´æ¥ä½¿ç”¨ os.getenv
            access_token = task.get('access_token')
            logger.info(f"[Task {task_id}] ğŸ” Token from task: {'YES (length: ' + str(len(access_token)) + ')' if access_token else 'NO'}")

            if not access_token:
                if task['repo_type'] == 'gitlab':
                    # å¯¹äº GitLabï¼Œä»ç¯å¢ƒå˜é‡è·å–å…¨å±€ token
                    access_token = os.getenv('GITLAB_TOKEN', '')
                    if access_token:
                        logger.info(f"[Task {task_id}] âœ… Using GITLAB_TOKEN from environment (length: {len(access_token)})")
                    else:
                        logger.warning(f"[Task {task_id}] âš ï¸ GITLAB_TOKEN not configured in environment")
                elif task['repo_type'] == 'github':
                    # å¯¹äº GitHubï¼Œä»ç¯å¢ƒå˜é‡è·å–å…¨å±€ token
                    access_token = os.getenv('GITHUB_TOKEN', '')
                    if access_token:
                        logger.info(f"[Task {task_id}] âœ… Using GITHUB_TOKEN from environment (length: {len(access_token)})")
                    else:
                        logger.warning(f"[Task {task_id}] âš ï¸ GITHUB_TOKEN not configured in environment")

            # Generate local path for repository
            adalflow_root = Path.home() / '.adalflow' / 'repos'
            repo_local_path = str(adalflow_root / task['owner'] / task['repo_name'])

            repo_path = download_repo(
                repo_url=task['repo_url'],
                local_path=repo_local_path,
                repo_type=task['repo_type'],
                access_token=access_token,  # â† ä½¿ç”¨ä»ç¯å¢ƒå˜é‡è·å–çš„ token
                force_refresh=task.get('force_refresh', False)  # â† ä»ä»»åŠ¡ä¸­è·å– force_refresh æ ‡å¿—
            )
            logger.info(f"[Task {task_id}] Repository cloned to {repo_path}")

            # ç¬¬2é˜¶æ®µï¼šæå–æ–‡æ¡£ï¼ˆè¿›åº¦20-40%ï¼‰
            logger.info(f"[Task {task_id}] Stage 2: Extracting documents...")
            self.db.update_task_status(
                task_id=task_id,
                status='processing',
                progress=20,
                message='Extracting documents from repository...'
            )

            excluded_dirs = task['excluded_dirs'].split('\n') if task['excluded_dirs'] else None
            excluded_files = task['excluded_files'].split('\n') if task['excluded_files'] else None
            included_dirs = task['included_dirs'].split('\n') if task['included_dirs'] else None
            included_files = task['included_files'].split('\n') if task['included_files'] else None

            documents = read_all_documents(
                path=repo_path,
                excluded_dirs=excluded_dirs,
                excluded_files=excluded_files,
                included_dirs=included_dirs,
                included_files=included_files
            )
            logger.info(f"[Task {task_id}] Extracted {len(documents)} documents")

            # ç¬¬3é˜¶æ®µï¼šç”Ÿæˆembeddingså’Œå‡†å¤‡retrieverï¼ˆè¿›åº¦40-70%ï¼‰
            logger.info(f"[Task {task_id}] Stage 3: Generating embeddings...")
            self.db.update_task_status(
                task_id=task_id,
                status='processing',
                progress=40,
                message=f'Generating embeddings for {len(documents)} documents...'
            )

            # ä½¿ç”¨é»˜è®¤å€¼å¦‚æœ provider æˆ– model ä¸ºç©ºï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
            from api.config import configs
            default_provider = configs.get('default_provider', 'google')
            default_model = configs.get('default_model', 'gemini-2.5-flash')
            provider = task['provider'] or default_provider
            model = task['model'] or default_model

            logger.info(f"[Task {task_id}] Config defaults - provider: {default_provider}, model: {default_model}")
            logger.info(f"[Task {task_id}] Task values - provider: {task.get('provider')}, model: {task.get('model')}")

            logger.info(f"[Task {task_id}] Using provider: {provider}, model: {model}")

            rag = RAG(
                provider=provider,
                model=model
            )

            # access_token å·²ç»åœ¨ Stage 1 ä»ç¯å¢ƒå˜é‡è·å–ï¼Œç›´æ¥ä½¿ç”¨
            logger.info(f"[Task {task_id}] ğŸ“¦ About to prepare retriever:")
            logger.info(f"[Task {task_id}]    - Repo URL: {task['repo_url']}")
            logger.info(f"[Task {task_id}]    - Repo Type: {task['repo_type']}")
            logger.info(f"[Task {task_id}]    - Has Token: {'YES' if access_token else 'NO'}")
            logger.info(f"[Task {task_id}]    - Excluded Dirs: {excluded_dirs[:50] if excluded_dirs else 'None'}")
            logger.info(f"[Task {task_id}]    - Excluded Files: {excluded_files[:50] if excluded_files else 'None'}")

            try:
                force_refresh = task.get('force_refresh', False)
                logger.info(f"[Task {task_id}] ğŸ”„ Force refresh: {force_refresh}")

                rag.prepare_retriever(
                    repo_url_or_path=task['repo_url'],
                    type=task['repo_type'],
                    access_token=access_token,
                    excluded_dirs=excluded_dirs,
                    excluded_files=excluded_files,
                    included_dirs=included_dirs,
                    included_files=included_files,
                    force_refresh=force_refresh
                )
                logger.info(f"[Task {task_id}] âœ… Retriever prepared successfully")
            except Exception as e:
                logger.error(f"[Task {task_id}] âŒ Failed to prepare retriever: {type(e).__name__}: {str(e)}")
                raise

            # ç¬¬4é˜¶æ®µï¼šç”Ÿæˆwikiç»“æ„å’Œå†…å®¹ï¼ˆè¿›åº¦70-95%ï¼‰
            logger.info(f"[Task {task_id}] Stage 4: Generating wiki structure and content...")
            self.db.update_task_status(
                task_id=task_id,
                status='processing',
                progress=70,
                message='Generating wiki structure...'
            )

            # è°ƒç”¨ç‹¬ç«‹çš„ wiki ç”Ÿæˆæ¨¡å—
            from api.wiki_generator import generate_wiki

            # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
            def update_wiki_progress(progress_pct, stage, detail_msg):
                """å›è°ƒå‡½æ•°ç”¨äºæ›´æ–°wikiç”Ÿæˆçš„ç»†ç²’åº¦è¿›åº¦"""
                self.db.update_task_status(
                    task_id=task_id,
                    status='processing',
                    progress=progress_pct,
                    message=detail_msg
                )

                # åŒæ—¶æ›´æ–°wiki_projectsè¡¨ä¸­çš„è¿›åº¦
                project_key = task.get('project_key')
                if project_key:
                    try:
                        with sqlite3.connect(self.db.db_path) as conn:
                            cursor = conn.cursor()
                            cursor.execute('''
                                UPDATE wiki_projects
                                SET progress = ?, message = ?
                                WHERE project_key = ?
                            ''', (progress_pct, detail_msg, project_key))
                            conn.commit()
                    except Exception as e:
                        logger.warning(f"[Task {task_id}] Failed to update wiki_projects progress: {e}")

                logger.info(f"[Task {task_id}] {stage}: {detail_msg}")

            wiki_structure, documents_count = generate_wiki(task, rag, progress_callback=update_wiki_progress)

            logger.info(f"[Task {task_id}] Wiki structure and content generated")

            # ç¬¬5é˜¶æ®µï¼šä¿å­˜ç»“æœå¹¶å®Œæˆï¼ˆè¿›åº¦95-100%ï¼‰
            logger.info(f"[Task {task_id}] Stage 5: Saving results and finalizing...")
            self.db.update_task_status(
                task_id=task_id,
                status='processing',
                progress=95,
                message='Saving wiki results...'
            )

            # è·å–æˆæœ¬ä¿¡æ¯
            cost_message = 'Wiki generation completed successfully!'
            try:
                from api.cost_tracker import get_cost_tracker, clear_cost_tracker
                cost_tracker = get_cost_tracker(task_id)
                cost_message = cost_tracker.get_cost_message()
                cost_tracker.log_summary()
                clear_cost_tracker(task_id)
            except Exception as e:
                logger.debug(f"[Task {task_id}] Could not get cost info: {e}")

            # ä¿å­˜ç»“æœåˆ°é¡¹ç›®è®°å½•
            project_key = task.get('project_key')
            if project_key:
                success = self.db.save_wiki_project_result(
                    project_key=project_key,
                    wiki_structure=wiki_structure,
                    documents_count=documents_count,
                    message=cost_message  # ä¼ å…¥æˆæœ¬æ¶ˆæ¯
                )

                if success:
                    logger.info(f"[Task {task_id}] Wiki results saved to project: {project_key}")

                    # ===== ä¿å­˜ Markdown é¡µé¢ï¼ˆä¸æ¸²æŸ“ HTMLï¼Œç”±å‰ç«¯å¤„ç†ï¼‰ =====
                    try:
                        logger.info(f"[Task {task_id}] Saving wiki pages as Markdown...")

                        # ä» wiki_structure æå–é¡µé¢çš„ markdown å†…å®¹
                        markdown_pages = {}
                        pages = wiki_structure.get('pages', [])

                        for page in pages:
                            page_id = page.get('id', '')
                            if not page_id:
                                logger.warning(f"[Task {task_id}] Skipping page without ID")
                                continue

                            markdown_pages[page_id] = {
                                'title': page.get('title', ''),
                                'markdown': page.get('content', ''),  # ç›´æ¥ä¿å­˜ markdown
                                'importance': page.get('importance', 'medium'),
                                'file_paths': page.get('filePaths', [])
                            }

                        # ä¿å­˜ markdown å†…å®¹ï¼ˆå¤ç”¨ save_rendered_pagesï¼Œä½†å­˜å‚¨çš„æ˜¯ markdownï¼‰
                        if markdown_pages:
                            save_success = self.db.save_markdown_pages(project_key, markdown_pages)
                            if save_success:
                                logger.info(f"âœ… [Task {task_id}] Saved {len(markdown_pages)} pages as Markdown")
                            else:
                                logger.warning(f"[Task {task_id}] Failed to save markdown pages")
                        else:
                            logger.warning(f"[Task {task_id}] No pages to save")

                    except Exception as e:
                        logger.error(f"[Task {task_id}] Error saving markdown pages: {e}", exc_info=True)
                        # ä¿å­˜å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ï¼Œç»§ç»­
                    # ===== ä¿å­˜ç»“æŸ =====
                else:
                    logger.warning(f"[Task {task_id}] Failed to save wiki results to project")
            else:
                logger.warning(f"[Task {task_id}] No project_key found, skipping project result save")

            # ä¿å­˜ç»“æœåˆ°ä»»åŠ¡è®°å½•
            self.db.update_task_status(
                task_id=task_id,
                status='completed',
                progress=100,
                message=cost_message,
                result={
                    'wiki_structure': wiki_structure,
                    'documents_count': documents_count,
                    'repo_path': repo_path
                }
            )

            # æ›´æ–°é¡¹ç›®çŠ¶æ€ä¸º generatedï¼Œå¹¶è®°å½•æˆåŠŸæ—¥æœŸ
            if project_key:
                today = datetime.now().strftime('%Y-%m-%d')
                self.db.update_wiki_project_status(project_key, 'generated', last_success_date=today)
                logger.info(f"[Task {task_id}] Project {project_key} marked as generated (date: {today})")

            logger.info(f"âœ… [Task {task_id}] ä»»åŠ¡å®Œæˆï¼ç”Ÿæˆäº† {len(wiki_structure['pages'])} ä¸ªé¡µé¢")

        except TaskTimeoutError as e:
            logger.error(f"â±ï¸ [Task {task_id}] ä»»åŠ¡æ‰§è¡Œè¶…æ—¶: {e.message}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.db.update_task_status(
                task_id=task_id,
                status='failed',
                progress=0,
                message='Task execution timeout',
                error_message=e.message
            )

            # æ›´æ–°é¡¹ç›®çŠ¶æ€ä¸º failed
            project_key = task.get('project_key')
            if project_key:
                self.db.update_wiki_project_status(project_key, 'failed')
                logger.info(f"[Task {task_id}] Project {project_key} marked as failed due to timeout")

        except Exception as e:
            logger.error(f"âŒ [Task {task_id}] ä»»åŠ¡å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.db.update_task_status(
                task_id=task_id,
                status='failed',
                progress=0,
                message='Task failed',
                error_message=str(e)
            )

            # æ›´æ–°é¡¹ç›®çŠ¶æ€ä¸º failed
            project_key = task.get('project_key')
            if project_key:
                self.db.update_wiki_project_status(project_key, 'failed')
                logger.info(f"[Task {task_id}] Project {project_key} marked as failed")

        finally:
            # ä»æ´»è·ƒä»»åŠ¡ä¸­ç§»é™¤
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
            logger.info(f"[Task {task_id}] Removed from active tasks")


# å…¨å±€ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨å®ä¾‹
_task_queue_manager: Optional[TaskQueueManager] = None


def get_task_queue_manager() -> TaskQueueManager:
    """è·å–å…¨å±€ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨å®ä¾‹"""
    global _task_queue_manager
    if _task_queue_manager is None:
        _task_queue_manager = TaskQueueManager()
    return _task_queue_manager


def init_task_queue():
    """åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—ï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
    # 1. æ¸…ç†æœåŠ¡å™¨é‡å¯æ—¶ä¸­æ–­çš„ä»»åŠ¡
    try:
        db = get_gitlab_db()
        cleaned_count = db.cleanup_interrupted_tasks()
        if cleaned_count > 0:
            logger.warning(f"âš ï¸ æœåŠ¡å™¨é‡å¯æ£€æµ‹åˆ° {cleaned_count} ä¸ªä¸­æ–­çš„ä»»åŠ¡ï¼Œå·²æ ‡è®°ä¸ºå¤±è´¥")
            logger.warning(f"ğŸ’¡ è¯·ç›¸å…³äººå‘˜é‡æ–°ç”Ÿæˆè¿™äº›é¡¹ç›®çš„ Wiki")
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†ä¸­æ–­ä»»åŠ¡å¤±è´¥: {str(e)}")

    # 2. å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨
    manager = get_task_queue_manager()
    manager.start()
    logger.info("âœ… ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨å·²åˆå§‹åŒ–å¹¶å¯åŠ¨")


def shutdown_task_queue():
    """å…³é—­ä»»åŠ¡é˜Ÿåˆ—ï¼ˆåœ¨åº”ç”¨å…³é—­æ—¶è°ƒç”¨ï¼‰"""
    global _task_queue_manager
    if _task_queue_manager:
        _task_queue_manager.stop()
        _task_queue_manager = None
    logger.info("ğŸ›‘ ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨å·²å…³é—­")
